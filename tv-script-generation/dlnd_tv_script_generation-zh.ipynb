{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 生成电视剧剧本\n",
    "\n",
    "在这个项目中，你将使用 RNN 创作你自己的[《辛普森一家》](https://zh.wikipedia.org/wiki/%E8%BE%9B%E6%99%AE%E6%A3%AE%E4%B8%80%E5%AE%B6)电视剧剧本。你将会用到《辛普森一家》第 27 季中部分剧本的[数据集](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data)。你创建的神经网络将为一个在 [Moe 酒馆](https://simpsonswiki.com/wiki/Moe's_Tavern)中的场景生成一集新的剧本。\n",
    "## 获取数据\n",
    "我们早已为你提供了数据。你将使用原始数据集的子集，它只包括 Moe 酒馆中的场景。数据中并不包括酒馆的其他版本，比如 “Moe 的山洞”、“燃烧的 Moe 酒馆”、“Moe 叔叔的家庭大餐”等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "\n",
    "data_dir = './data/simpsons/moes_tavern_lines.txt'\n",
    "text = helper.load_data(data_dir)\n",
    "# Ignore notice, since we don't use it for analysing the data\n",
    "text = text[81:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 探索数据\n",
    "使用 `view_sentence_range` 来查看数据的不同部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 11492\n",
      "Number of scenes: 262\n",
      "Average number of sentences in each scene: 15.248091603053435\n",
      "Number of lines: 4257\n",
      "Average number of words in each line: 11.50434578341555\n",
      "\n",
      "The sentences 0 to 10:\n",
      "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
      "Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.\n",
      "Moe_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?\n",
      "Moe_Szyslak: (INTO PHONE) Listen you little puke. One of these days I'm gonna catch you, and I'm gonna carve my name on your back with an ice pick.\n",
      "Moe_Szyslak: What's the matter Homer? You're not your normal effervescent self.\n",
      "Homer_Simpson: I got my problems, Moe. Give me another one.\n",
      "Moe_Szyslak: Homer, hey, you should not drink to forget your problems.\n",
      "Barney_Gumble: Yeah, you should only drink to enhance your social skills.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 实现预处理函数\n",
    "对数据集进行的第一个操作是预处理。请实现下面两个预处理函数：\n",
    "\n",
    "- 查询表\n",
    "- 标记符号的字符串\n",
    "\n",
    "### 查询表\n",
    "要创建词嵌入，你首先要将词语转换为 id。请在这个函数中创建两个字典：\n",
    "\n",
    "- 将词语转换为 id 的字典，我们称它为 `vocab_to_int`\n",
    "- 将 id 转换为词语的字典，我们称它为 `int_to_vocab`\n",
    "\n",
    "请在下面的元组中返回这些字典\n",
    " `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    word_counts = Counter(text)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 标记符号的字符串\n",
    "我们会使用空格当作分隔符，来将剧本分割为词语数组。然而，句号和感叹号等符号使得神经网络难以分辨“再见”和“再见！”之间的区别。\n",
    "\n",
    "实现函数 `token_lookup` 来返回一个字典，这个字典用于将 “!” 等符号标记为 “||Exclamation_Mark||” 形式。为下列符号创建一个字典，其中符号为标志，值为标记。\n",
    "\n",
    "- period ( . )\n",
    "- comma ( , )\n",
    "- quotation mark ( \" )\n",
    "- semicolon ( ; )\n",
    "- exclamation mark ( ! )\n",
    "- question mark ( ? )\n",
    "- left parenthesis ( ( )\n",
    "- right parenthesis ( ) )\n",
    "- dash ( -- )\n",
    "- return ( \\n )\n",
    "\n",
    "这个字典将用于标记符号并在其周围添加分隔符（空格）。这能将符号视作单独词汇分割开来，并使神经网络更轻松地预测下一个词汇。请确保你并没有使用容易与词汇混淆的标记。与其使用 “dash” 这样的标记，试试使用“||dash||”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    token_look_table = {}\n",
    "    token_look_table['.'] = \"||period||\"\n",
    "    token_look_table[','] = \"||comma||\"\n",
    "    token_look_table['\"'] = \"||quotation_mark||\"\n",
    "    token_look_table[';'] = \"||semicolon||\"\n",
    "    token_look_table['!'] = \"||exclamation_mark||\"\n",
    "    token_look_table['?'] = \"||question_mark||\"\n",
    "    token_look_table['('] = \"||left_parenthesis||\"\n",
    "    token_look_table[')'] = \"||right_parenthesis||\"\n",
    "    token_look_table['--'] = \"||dash||\"\n",
    "    token_look_table['\\n'] = \"||return||\"\n",
    "    return token_look_table\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 预处理并保存所有数据\n",
    "运行以下代码将预处理所有数据，并将它们保存至文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 检查点\n",
    "这是你遇到的第一个检点。如果你想要回到这个 notebook，或需要重新打开 notebook，你都可以从这里开始。预处理的数据都已经保存完毕。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '||period||',\n",
       " 1: '||return||',\n",
       " 2: '||comma||',\n",
       " 3: '||left_parenthesis||',\n",
       " 4: '||right_parenthesis||',\n",
       " 5: 'the',\n",
       " 6: 'i',\n",
       " 7: 'you',\n",
       " 8: '||exclamation_mark||',\n",
       " 9: 'moe_szyslak:',\n",
       " 10: '||question_mark||',\n",
       " 11: 'a',\n",
       " 12: 'homer_simpson:',\n",
       " 13: 'to',\n",
       " 14: 'and',\n",
       " 15: 'of',\n",
       " 16: 'my',\n",
       " 17: 'it',\n",
       " 18: 'that',\n",
       " 19: 'in',\n",
       " 20: '||quotation_mark||',\n",
       " 21: 'me',\n",
       " 22: 'is',\n",
       " 23: 'this',\n",
       " 24: \"i'm\",\n",
       " 25: 'for',\n",
       " 26: 'your',\n",
       " 27: 'homer',\n",
       " 28: 'hey',\n",
       " 29: 'on',\n",
       " 30: 'moe',\n",
       " 31: 'oh',\n",
       " 32: 'no',\n",
       " 33: 'lenny_leonard:',\n",
       " 34: 'what',\n",
       " 35: 'with',\n",
       " 36: 'yeah',\n",
       " 37: 'all',\n",
       " 38: 'just',\n",
       " 39: 'like',\n",
       " 40: 'but',\n",
       " 41: 'barney_gumble:',\n",
       " 42: 'so',\n",
       " 43: 'be',\n",
       " 44: 'here',\n",
       " 45: 'carl_carlson:',\n",
       " 46: \"don't\",\n",
       " 47: 'have',\n",
       " 48: \"it's\",\n",
       " 49: 'up',\n",
       " 50: 'well',\n",
       " 51: 'out',\n",
       " 52: 'do',\n",
       " 53: 'was',\n",
       " 54: 'got',\n",
       " 55: 'are',\n",
       " 56: 'get',\n",
       " 57: 'we',\n",
       " 58: 'uh',\n",
       " 59: \"that's\",\n",
       " 60: 'one',\n",
       " 61: \"you're\",\n",
       " 62: 'not',\n",
       " 63: 'now',\n",
       " 64: 'know',\n",
       " 65: 'can',\n",
       " 66: '||dash||',\n",
       " 67: 'at',\n",
       " 68: 'right',\n",
       " 69: '/',\n",
       " 70: 'how',\n",
       " 71: 'if',\n",
       " 72: 'back',\n",
       " 73: 'marge_simpson:',\n",
       " 74: 'about',\n",
       " 75: 'from',\n",
       " 76: 'he',\n",
       " 77: 'go',\n",
       " 78: 'gonna',\n",
       " 79: 'there',\n",
       " 80: 'they',\n",
       " 81: 'beer',\n",
       " 82: 'good',\n",
       " 83: 'who',\n",
       " 84: 'an',\n",
       " 85: 'man',\n",
       " 86: 'okay',\n",
       " 87: 'his',\n",
       " 88: 'little',\n",
       " 89: 'as',\n",
       " 90: 'some',\n",
       " 91: 'then',\n",
       " 92: \"can't\",\n",
       " 93: 'never',\n",
       " 94: 'come',\n",
       " 95: 'think',\n",
       " 96: \"i'll\",\n",
       " 97: 'could',\n",
       " 98: 'him',\n",
       " 99: \"i've\",\n",
       " 100: 'look',\n",
       " 101: 'really',\n",
       " 102: 'see',\n",
       " 103: 'want',\n",
       " 104: 'too',\n",
       " 105: 'guys',\n",
       " 106: 'been',\n",
       " 107: 'when',\n",
       " 108: 'make',\n",
       " 109: 'why',\n",
       " 110: 'ya',\n",
       " 111: 'bar',\n",
       " 112: 'her',\n",
       " 113: 'did',\n",
       " 114: 'time',\n",
       " 115: 'say',\n",
       " 116: 'or',\n",
       " 117: 'ah',\n",
       " 118: 'gotta',\n",
       " 119: 'marge',\n",
       " 120: 'take',\n",
       " 121: 'love',\n",
       " 122: 'into',\n",
       " 123: 'down',\n",
       " 124: 'more',\n",
       " 125: 'our',\n",
       " 126: 'am',\n",
       " 127: 'off',\n",
       " 128: 'guy',\n",
       " 129: 'sure',\n",
       " 130: 'two',\n",
       " 131: 'barney',\n",
       " 132: \"there's\",\n",
       " 133: 'thing',\n",
       " 134: 'would',\n",
       " 135: 'lisa_simpson:',\n",
       " 136: \"we're\",\n",
       " 137: 'need',\n",
       " 138: 'tell',\n",
       " 139: 'big',\n",
       " 140: 'had',\n",
       " 141: \"he's\",\n",
       " 142: 'where',\n",
       " 143: 'let',\n",
       " 144: 'money',\n",
       " 145: 'over',\n",
       " 146: 'sorry',\n",
       " 147: \"what's\",\n",
       " 148: 'something',\n",
       " 149: 'us',\n",
       " 150: 'bart_simpson:',\n",
       " 151: 'drink',\n",
       " 152: 'by',\n",
       " 153: 'only',\n",
       " 154: 'ever',\n",
       " 155: 'way',\n",
       " 156: 'day',\n",
       " 157: 'will',\n",
       " 158: 'wait',\n",
       " 159: 'she',\n",
       " 160: 'chief_wiggum:',\n",
       " 161: 'even',\n",
       " 162: 'give',\n",
       " 163: \"i'd\",\n",
       " 164: 'new',\n",
       " 165: 'god',\n",
       " 166: 'huh',\n",
       " 167: 'those',\n",
       " 168: \"ain't\",\n",
       " 169: \"didn't\",\n",
       " 170: 'people',\n",
       " 171: 'great',\n",
       " 172: 'lenny',\n",
       " 173: 'maybe',\n",
       " 174: 'has',\n",
       " 175: 'much',\n",
       " 176: 'life',\n",
       " 177: 'eh',\n",
       " 178: \"moe's\",\n",
       " 179: 'phone',\n",
       " 180: 'were',\n",
       " 181: 'mean',\n",
       " 182: 'than',\n",
       " 183: 'going',\n",
       " 184: 'place',\n",
       " 185: 'around',\n",
       " 186: 'should',\n",
       " 187: 'mr',\n",
       " 188: \"you've\",\n",
       " 189: 'these',\n",
       " 190: 'wanna',\n",
       " 191: 'still',\n",
       " 192: 'better',\n",
       " 193: 'help',\n",
       " 194: 'home',\n",
       " 195: 'old',\n",
       " 196: \"'em\",\n",
       " 197: 'friend',\n",
       " 198: 'night',\n",
       " 199: 'name',\n",
       " 200: 'noise',\n",
       " 201: 'before',\n",
       " 202: 'please',\n",
       " 203: 'seymour_skinner:',\n",
       " 204: 'last',\n",
       " 205: 'aw',\n",
       " 206: 'whoa',\n",
       " 207: 'tv',\n",
       " 208: 'made',\n",
       " 209: 'any',\n",
       " 210: 'face',\n",
       " 211: 'boy',\n",
       " 212: 'thanks',\n",
       " 213: \"'cause\",\n",
       " 214: 'duff',\n",
       " 215: 'three',\n",
       " 216: 'hello',\n",
       " 217: 'put',\n",
       " 218: 'drunk',\n",
       " 219: 'call',\n",
       " 220: 'again',\n",
       " 221: 'listen',\n",
       " 222: 'looking',\n",
       " 223: 'car',\n",
       " 224: 'their',\n",
       " 225: 'bad',\n",
       " 226: 'best',\n",
       " 227: \"let's\",\n",
       " 228: 'first',\n",
       " 229: 'very',\n",
       " 230: 'yes',\n",
       " 231: 'wow',\n",
       " 232: 'does',\n",
       " 233: 'every',\n",
       " 234: 'kent_brockman:',\n",
       " 235: 'another',\n",
       " 236: 'ooh',\n",
       " 237: 'them',\n",
       " 238: 'said',\n",
       " 239: 'while',\n",
       " 240: 'looks',\n",
       " 241: 'other',\n",
       " 242: 'apu_nahasapeemapetilon:',\n",
       " 243: 'wife',\n",
       " 244: 'guess',\n",
       " 245: 'work',\n",
       " 246: 'years',\n",
       " 247: 'springfield',\n",
       " 248: 'tonight',\n",
       " 249: 'dad',\n",
       " 250: 'sweet',\n",
       " 251: 'feel',\n",
       " 252: 'singing',\n",
       " 253: \"won't\",\n",
       " 254: 'play',\n",
       " 255: 'find',\n",
       " 256: 'thought',\n",
       " 257: 'voice',\n",
       " 258: 'after',\n",
       " 259: 'everybody',\n",
       " 260: \"they're\",\n",
       " 261: 'sobs',\n",
       " 262: 'dr',\n",
       " 263: 'things',\n",
       " 264: 'kids',\n",
       " 265: 'buy',\n",
       " 266: 'might',\n",
       " 267: 'girl',\n",
       " 268: 'shut',\n",
       " 269: 'head',\n",
       " 270: 'because',\n",
       " 271: 'since',\n",
       " 272: 'nice',\n",
       " 273: 'check',\n",
       " 274: 'minute',\n",
       " 275: 'keep',\n",
       " 276: 'happy',\n",
       " 277: 'beat',\n",
       " 278: 'show',\n",
       " 279: 'world',\n",
       " 280: 'sings',\n",
       " 281: 'chuckle',\n",
       " 282: 'bart',\n",
       " 283: 'always',\n",
       " 284: \"who's\",\n",
       " 285: 'use',\n",
       " 286: 'lisa',\n",
       " 287: 'sighs',\n",
       " 288: 'friends',\n",
       " 289: 'stupid',\n",
       " 290: 'someone',\n",
       " 291: \"isn't\",\n",
       " 292: 'c',\n",
       " 293: 'kid',\n",
       " 294: 'which',\n",
       " 295: 'ow',\n",
       " 296: 'krusty_the_clown:',\n",
       " 297: \"you'll\",\n",
       " 298: 'carl',\n",
       " 299: 'hundred',\n",
       " 300: 'talk',\n",
       " 301: 'anything',\n",
       " 302: 'remember',\n",
       " 303: 'laugh',\n",
       " 304: \"here's\",\n",
       " 305: 'seen',\n",
       " 306: 'lot',\n",
       " 307: 'next',\n",
       " 308: 'glass',\n",
       " 309: 'chuckles',\n",
       " 310: 'thank',\n",
       " 311: 'through',\n",
       " 312: 'laughs',\n",
       " 313: 'simpson',\n",
       " 314: 'job',\n",
       " 315: 'hell',\n",
       " 316: 'says',\n",
       " 317: \"nothin'\",\n",
       " 318: 'woman',\n",
       " 319: 'believe',\n",
       " 320: 'hear',\n",
       " 321: 'lost',\n",
       " 322: 'house',\n",
       " 323: 'outta',\n",
       " 324: 'long',\n",
       " 325: 'pretty',\n",
       " 326: 'away',\n",
       " 327: 'hope',\n",
       " 328: 'happened',\n",
       " 329: 'five',\n",
       " 330: 'kind',\n",
       " 331: 'matter',\n",
       " 332: 'nervous',\n",
       " 333: 'stop',\n",
       " 334: 'family',\n",
       " 335: 'tavern',\n",
       " 336: 'real',\n",
       " 337: 'fat',\n",
       " 338: 'once',\n",
       " 339: \"c'mon\",\n",
       " 340: 'comes',\n",
       " 341: '_montgomery_burns:',\n",
       " 342: 'turn',\n",
       " 343: 'wish',\n",
       " 344: \"homer's\",\n",
       " 345: 'book',\n",
       " 346: 'four',\n",
       " 347: 'ned_flanders:',\n",
       " 348: 'waylon_smithers:',\n",
       " 349: \"doin'\",\n",
       " 350: 'wrong',\n",
       " 351: 'myself',\n",
       " 352: 'wants',\n",
       " 353: 'business',\n",
       " 354: 'actually',\n",
       " 355: \"goin'\",\n",
       " 356: 'grampa_simpson:',\n",
       " 357: 'ask',\n",
       " 358: 'idea',\n",
       " 359: 'game',\n",
       " 360: 'everything',\n",
       " 361: \"comin'\",\n",
       " 362: 'used',\n",
       " 363: 'party',\n",
       " 364: 'done',\n",
       " 365: \"wouldn't\",\n",
       " 366: 'enough',\n",
       " 367: 'loud',\n",
       " 368: 'today',\n",
       " 369: 'getting',\n",
       " 370: \"we've\",\n",
       " 371: 'burns',\n",
       " 372: 'problem',\n",
       " 373: 'must',\n",
       " 374: 'duffman:',\n",
       " 375: 'nobody',\n",
       " 376: 'town',\n",
       " 377: \"she's\",\n",
       " 378: 'many',\n",
       " 379: 'na',\n",
       " 380: 'watch',\n",
       " 381: 'doing',\n",
       " 382: 'woo',\n",
       " 383: 'nah',\n",
       " 384: 'bucks',\n",
       " 385: 'try',\n",
       " 386: 'hold',\n",
       " 387: 'reading',\n",
       " 388: 'free',\n",
       " 389: 'gee',\n",
       " 390: 'dollars',\n",
       " 391: 'um',\n",
       " 392: 'true',\n",
       " 393: 'sounds',\n",
       " 394: 'took',\n",
       " 395: 'maggie',\n",
       " 396: 'thinking',\n",
       " 397: 'sound',\n",
       " 398: 'pay',\n",
       " 399: 'gimme',\n",
       " 400: 'baby',\n",
       " 401: 'excuse',\n",
       " 402: 'leave',\n",
       " 403: 'chief',\n",
       " 404: 'being',\n",
       " 405: 'stuff',\n",
       " 406: 'canyonero',\n",
       " 407: 'pants',\n",
       " 408: 'wanted',\n",
       " 409: 'secret',\n",
       " 410: 'sell',\n",
       " 411: 'makes',\n",
       " 412: 'beautiful',\n",
       " 413: 'under',\n",
       " 414: 'edna',\n",
       " 415: 'daughter',\n",
       " 416: 'everyone',\n",
       " 417: 'most',\n",
       " 418: 'yourself',\n",
       " 419: 'care',\n",
       " 420: 'kill',\n",
       " 421: 'kemi:',\n",
       " 422: \"where's\",\n",
       " 423: 'left',\n",
       " 424: 'dinner',\n",
       " 425: 'sad',\n",
       " 426: 'pick',\n",
       " 427: 'pal',\n",
       " 428: 'smithers',\n",
       " 429: \"you'd\",\n",
       " 430: 'own',\n",
       " 431: 'dead',\n",
       " 432: 'knew',\n",
       " 433: 'quickly',\n",
       " 434: 'hurt',\n",
       " 435: 'tough',\n",
       " 436: 'worry',\n",
       " 437: 'tipsy',\n",
       " 438: 'went',\n",
       " 439: 'points',\n",
       " 440: 'save',\n",
       " 441: 'mouth',\n",
       " 442: 'till',\n",
       " 443: 'hate',\n",
       " 444: 'heard',\n",
       " 445: 'ladies',\n",
       " 446: 'break',\n",
       " 447: 'funny',\n",
       " 448: 'sign',\n",
       " 449: 'excited',\n",
       " 450: 'gave',\n",
       " 451: 'die',\n",
       " 452: 'hi',\n",
       " 453: 'told',\n",
       " 454: 'came',\n",
       " 455: 'tomorrow',\n",
       " 456: 'school',\n",
       " 457: 'skinner',\n",
       " 458: 'feeling',\n",
       " 459: 'dog',\n",
       " 460: 'camera',\n",
       " 461: 'saw',\n",
       " 462: 'booze',\n",
       " 463: 'hoo',\n",
       " 464: 'quit',\n",
       " 465: 'drinking',\n",
       " 466: 'win',\n",
       " 467: 'eyes',\n",
       " 468: 'fine',\n",
       " 469: 'gets',\n",
       " 470: 'jacques:',\n",
       " 471: 'hand',\n",
       " 472: 'room',\n",
       " 473: 'kinda',\n",
       " 474: 'anyone',\n",
       " 475: 'clean',\n",
       " 476: 'without',\n",
       " 477: 'easy',\n",
       " 478: 'forget',\n",
       " 479: 'nuts',\n",
       " 480: 'mad',\n",
       " 481: 'hands',\n",
       " 482: 'loves',\n",
       " 483: 'seven',\n",
       " 484: 'barflies:',\n",
       " 485: 'noises',\n",
       " 486: 'burn',\n",
       " 487: 'bring',\n",
       " 488: 'fight',\n",
       " 489: 'read',\n",
       " 490: 'sir',\n",
       " 491: 'eat',\n",
       " 492: 'krusty',\n",
       " 493: \"couldn't\",\n",
       " 494: 'mom',\n",
       " 495: 'gone',\n",
       " 496: 'flaming',\n",
       " 497: 'super',\n",
       " 498: 'drive',\n",
       " 499: 'alcohol',\n",
       " 500: 'million',\n",
       " 501: 'calling',\n",
       " 502: 'twenty',\n",
       " 503: 'gasp',\n",
       " 504: 'surprised',\n",
       " 505: 'cash',\n",
       " 506: 'fire',\n",
       " 507: 'date',\n",
       " 508: 'kirk_van_houten:',\n",
       " 509: 'small',\n",
       " 510: 'artie_ziff:',\n",
       " 511: \"aren't\",\n",
       " 512: 'loser',\n",
       " 513: 'course',\n",
       " 514: 'low',\n",
       " 515: 'trouble',\n",
       " 516: 'seymour',\n",
       " 517: 'six',\n",
       " 518: 'cut',\n",
       " 519: 'high',\n",
       " 520: 'already',\n",
       " 521: 'gentlemen',\n",
       " 522: \"lookin'\",\n",
       " 523: 'turns',\n",
       " 524: 'chance',\n",
       " 525: \"haven't\",\n",
       " 526: 'door',\n",
       " 527: 'meet',\n",
       " 528: 'coming',\n",
       " 529: 'geez',\n",
       " 530: \"wasn't\",\n",
       " 531: \"we'll\",\n",
       " 532: \"drinkin'\",\n",
       " 533: 'happen',\n",
       " 534: \"doesn't\",\n",
       " 535: 'bartender',\n",
       " 536: 'behind',\n",
       " 537: 'dear',\n",
       " 538: 'tape',\n",
       " 539: 'problems',\n",
       " 540: 'sadly',\n",
       " 541: \"somethin'\",\n",
       " 542: \"talkin'\",\n",
       " 543: 'each',\n",
       " 544: 'professor_jonathan_frink:',\n",
       " 545: 'anyway',\n",
       " 546: \"y'know\",\n",
       " 547: 'upset',\n",
       " 548: 'called',\n",
       " 549: 'eye',\n",
       " 550: 'stay',\n",
       " 551: 'watching',\n",
       " 552: 'talking',\n",
       " 553: 'straight',\n",
       " 554: 'blame',\n",
       " 555: 'sing',\n",
       " 556: 'self',\n",
       " 557: 'war',\n",
       " 558: \"gettin'\",\n",
       " 559: 'fun',\n",
       " 560: 'sigh',\n",
       " 561: 'heart',\n",
       " 562: 'worried',\n",
       " 563: \"it'll\",\n",
       " 564: 'stand',\n",
       " 565: 'ugly',\n",
       " 566: 'snake_jailbird:',\n",
       " 567: 'spend',\n",
       " 568: 'uh-oh',\n",
       " 569: 'hot',\n",
       " 570: 'start',\n",
       " 571: 'whole',\n",
       " 572: 'keys',\n",
       " 573: 'close',\n",
       " 574: 'whatever',\n",
       " 575: 'quiet',\n",
       " 576: 'learn',\n",
       " 577: 'wiggum',\n",
       " 578: 'change',\n",
       " 579: 'disgusted',\n",
       " 580: 'worse',\n",
       " 581: 'lady',\n",
       " 582: 'least',\n",
       " 583: 'mmmm',\n",
       " 584: 'song',\n",
       " 585: 'bottle',\n",
       " 586: 'moan',\n",
       " 587: 'bar_rag:',\n",
       " 588: 'end',\n",
       " 589: 'outside',\n",
       " 590: 'dump',\n",
       " 591: 'machine',\n",
       " 592: ':',\n",
       " 593: 'works',\n",
       " 594: 'the_rich_texan:',\n",
       " 595: 'trying',\n",
       " 596: 'larry:',\n",
       " 597: 'steal',\n",
       " 598: 'crazy',\n",
       " 599: 'realizing',\n",
       " 600: 'm',\n",
       " 601: 'live',\n",
       " 602: 'throat',\n",
       " 603: 'hours',\n",
       " 604: 'perfect',\n",
       " 605: 'thousand',\n",
       " 606: 'greatest',\n",
       " 607: 'blue',\n",
       " 608: 'ma',\n",
       " 609: 'gasps',\n",
       " 610: 'morning',\n",
       " 611: 'such',\n",
       " 612: 'goodbye',\n",
       " 613: 'bit',\n",
       " 614: 'may',\n",
       " 615: 'shocked',\n",
       " 616: \"shouldn't\",\n",
       " 617: 'annoyed',\n",
       " 618: 'playing',\n",
       " 619: 'soon',\n",
       " 620: 'apu',\n",
       " 621: 'second',\n",
       " 622: 'heh',\n",
       " 623: 'alive',\n",
       " 624: 'pull',\n",
       " 625: 'either',\n",
       " 626: 'learned',\n",
       " 627: 'turned',\n",
       " 628: 'married',\n",
       " 629: 'drinks',\n",
       " 630: 'police',\n",
       " 631: 'barflies',\n",
       " 632: 'tsk',\n",
       " 633: 'special',\n",
       " 634: 'less',\n",
       " 635: 'private',\n",
       " 636: 'year',\n",
       " 637: 'front',\n",
       " 638: 'far',\n",
       " 639: 'young',\n",
       " 640: 'marriage',\n",
       " 641: 'anymore',\n",
       " 642: 'light',\n",
       " 643: 'mother',\n",
       " 644: 'times',\n",
       " 645: 'crowd:',\n",
       " 646: 'whip',\n",
       " 647: 'probably',\n",
       " 648: 'air',\n",
       " 649: 'late',\n",
       " 650: 'point',\n",
       " 651: 'girls',\n",
       " 652: 'joe',\n",
       " 653: 'stick',\n",
       " 654: 'lousy',\n",
       " 655: 'delete',\n",
       " 656: 'person',\n",
       " 657: 'thinks',\n",
       " 658: \"'bout\",\n",
       " 659: 'poor',\n",
       " 660: 'ten',\n",
       " 661: 'butt',\n",
       " 662: 'bowl',\n",
       " 663: 'else',\n",
       " 664: 'smell',\n",
       " 665: 'mayor_joe_quimby:',\n",
       " 666: 'boxing',\n",
       " 667: 'rev',\n",
       " 668: 'throw',\n",
       " 669: 'moron',\n",
       " 670: 'later',\n",
       " 671: 'mind',\n",
       " 672: 'found',\n",
       " 673: 'serious',\n",
       " 674: 'tab',\n",
       " 675: 'blood',\n",
       " 676: 'anybody',\n",
       " 677: 'lemme',\n",
       " 678: 'store',\n",
       " 679: 'ticket',\n",
       " 680: 'president',\n",
       " 681: 'letter',\n",
       " 682: 'boys',\n",
       " 683: 'eight',\n",
       " 684: 'feet',\n",
       " 685: 'king',\n",
       " 686: 'kick',\n",
       " 687: 'fifty',\n",
       " 688: 'christmas',\n",
       " 689: 'instead',\n",
       " 690: 'both',\n",
       " 691: 'open',\n",
       " 692: 'beers',\n",
       " 693: 'youse',\n",
       " 694: 'lucky',\n",
       " 695: 'turning',\n",
       " 696: 'shotgun',\n",
       " 697: 'pig',\n",
       " 698: 'uh-huh',\n",
       " 699: 'shot',\n",
       " 700: 'picture',\n",
       " 701: 'couple',\n",
       " 702: 'box',\n",
       " 703: 'cool',\n",
       " 704: 'minutes',\n",
       " 705: 'knows',\n",
       " 706: 'yet',\n",
       " 707: 'angry',\n",
       " 708: 'agnes_skinner:',\n",
       " 709: 'arm',\n",
       " 710: 'card',\n",
       " 711: 'jacques',\n",
       " 712: 'duffman',\n",
       " 713: 'join',\n",
       " 714: 'exactly',\n",
       " 715: 'goes',\n",
       " 716: 'alone',\n",
       " 717: 'patty_bouvier:',\n",
       " 718: 'goodnight',\n",
       " 719: 'buddy',\n",
       " 720: 'nods',\n",
       " 721: 'nothing',\n",
       " 722: 'ass',\n",
       " 723: \"how'd\",\n",
       " 724: 'barn',\n",
       " 725: 'miss',\n",
       " 726: 'taking',\n",
       " 727: 'american',\n",
       " 728: 'walk',\n",
       " 729: 'means',\n",
       " 730: 'round',\n",
       " 731: 'mrs',\n",
       " 732: 'street',\n",
       " 733: 'send',\n",
       " 734: '||semicolon||',\n",
       " 735: 'wire',\n",
       " 736: 'dance',\n",
       " 737: 'collette:',\n",
       " 738: 'grampa',\n",
       " 739: 'selma_bouvier:',\n",
       " 740: 'english',\n",
       " 741: 'nine',\n",
       " 742: 'ahh',\n",
       " 743: \"he'll\",\n",
       " 744: 'plant',\n",
       " 745: 'etc',\n",
       " 746: 'news',\n",
       " 747: 'making',\n",
       " 748: 'fellas',\n",
       " 749: 'glove',\n",
       " 750: 'huge',\n",
       " 751: 'moans',\n",
       " 752: 'son',\n",
       " 753: 'rummy',\n",
       " 754: 'warmly',\n",
       " 755: 'happier',\n",
       " 756: 'homie',\n",
       " 757: 'somebody',\n",
       " 758: 'terrible',\n",
       " 759: 'fast',\n",
       " 760: 'narrator:',\n",
       " 761: 'screw',\n",
       " 762: 'ned',\n",
       " 763: 'o',\n",
       " 764: 'together',\n",
       " 765: \"makin'\",\n",
       " 766: 'crap',\n",
       " 767: 'plus',\n",
       " 768: 'walking',\n",
       " 769: 'friendly',\n",
       " 770: 'kiss',\n",
       " 771: 'tap',\n",
       " 772: 'smile',\n",
       " 773: 'amazed',\n",
       " 774: 'cold',\n",
       " 775: 'sitting',\n",
       " 776: 'words',\n",
       " 777: 'move',\n",
       " 778: 'ready',\n",
       " 779: 'horrible',\n",
       " 780: 'weird',\n",
       " 781: 'losers',\n",
       " 782: 'seat',\n",
       " 783: 'intrigued',\n",
       " 784: 'pour',\n",
       " 785: 'alright',\n",
       " 786: 'scared',\n",
       " 787: 'days',\n",
       " 788: 'broke',\n",
       " 789: 'number',\n",
       " 790: 'advice',\n",
       " 791: 'sotto',\n",
       " 792: 'favorite',\n",
       " 793: 'butts',\n",
       " 794: 'saying',\n",
       " 795: 's',\n",
       " 796: 'full',\n",
       " 797: 'young_marge:',\n",
       " 798: 'inside',\n",
       " 799: 'warm_female_voice:',\n",
       " 800: 'run',\n",
       " 801: 'using',\n",
       " 802: 'black',\n",
       " 803: \"kiddin'\",\n",
       " 804: 'eggs',\n",
       " 805: 'glad',\n",
       " 806: 'welcome',\n",
       " 807: 'ball',\n",
       " 808: 'top',\n",
       " 809: 'dry',\n",
       " 810: 'lucius:',\n",
       " 811: 'paint',\n",
       " 812: 'hide',\n",
       " 813: 'himself',\n",
       " 814: 'story',\n",
       " 815: 'ha',\n",
       " 816: 'return',\n",
       " 817: 'accident',\n",
       " 818: 'city',\n",
       " 819: 'channel',\n",
       " 820: 'food',\n",
       " 821: 'table',\n",
       " 822: 'japanese',\n",
       " 823: 'forever',\n",
       " 824: 'comic_book_guy:',\n",
       " 825: 'though',\n",
       " 826: 'hang',\n",
       " 827: 'la',\n",
       " 828: 'finally',\n",
       " 829: 'word',\n",
       " 830: 'seems',\n",
       " 831: \"ol'\",\n",
       " 832: 'proudly',\n",
       " 833: 'peanuts',\n",
       " 834: 'human',\n",
       " 835: 'afraid',\n",
       " 836: 'hmm',\n",
       " 837: 'deal',\n",
       " 838: 'renee:',\n",
       " 839: 'telling',\n",
       " 840: 'damn',\n",
       " 841: 'health_inspector:',\n",
       " 842: 'dumb',\n",
       " 843: 'bet',\n",
       " 844: 'bitter',\n",
       " 845: 'lives',\n",
       " 846: 'hair',\n",
       " 847: 'along',\n",
       " 848: 'plastic',\n",
       " 849: 'rat',\n",
       " 850: 'fill',\n",
       " 851: 'music',\n",
       " 852: 'water',\n",
       " 853: 'pass',\n",
       " 854: 'sex',\n",
       " 855: 'truth',\n",
       " 856: 'lou:',\n",
       " 857: 'health',\n",
       " 858: 'won',\n",
       " 859: 'crack',\n",
       " 860: 'fall',\n",
       " 861: 'quick',\n",
       " 862: 'harv:',\n",
       " 863: 'treasure',\n",
       " 864: 'nigel_bakerbutcher:',\n",
       " 865: 'tune',\n",
       " 866: 'state',\n",
       " 867: \"that'll\",\n",
       " 868: 'romantic',\n",
       " 869: 'principal',\n",
       " 870: 'szyslak',\n",
       " 871: 'young_homer:',\n",
       " 872: 'dunno',\n",
       " 873: 'hit',\n",
       " 874: 'al',\n",
       " 875: 'ones',\n",
       " 876: 'sent',\n",
       " 877: \"we'd\",\n",
       " 878: 'football_announcer:',\n",
       " 879: 'honest',\n",
       " 880: 'baseball',\n",
       " 881: 'numbers',\n",
       " 882: 'strong',\n",
       " 883: 'princess',\n",
       " 884: 'fat_tony:',\n",
       " 885: 'touched',\n",
       " 886: 'group',\n",
       " 887: 'men',\n",
       " 888: 'plan',\n",
       " 889: 'lowers',\n",
       " 890: 'stool',\n",
       " 891: 'eating',\n",
       " 892: 'takes',\n",
       " 893: 'across',\n",
       " 894: 'smells',\n",
       " 895: 'company',\n",
       " 896: 'jukebox',\n",
       " 897: 'forgot',\n",
       " 898: 'denver',\n",
       " 899: 'half',\n",
       " 900: 'power',\n",
       " 901: 'admit',\n",
       " 902: '_julius_hibbert:',\n",
       " 903: 'ice',\n",
       " 904: 'desperate',\n",
       " 905: 'holding',\n",
       " 906: 'tony',\n",
       " 907: 'kent',\n",
       " 908: 'set',\n",
       " 909: 'slap',\n",
       " 910: 'walther_hotenhoffer:',\n",
       " 911: 'invented',\n",
       " 912: 'honey',\n",
       " 913: 'worst',\n",
       " 914: 'hard',\n",
       " 915: 'gives',\n",
       " 916: 'uncle',\n",
       " 917: 'lying',\n",
       " 918: 'safe',\n",
       " 919: 'write',\n",
       " 920: 'laughing',\n",
       " 921: 'bag',\n",
       " 922: 'loved',\n",
       " 923: 'dangerous',\n",
       " 924: 'slow',\n",
       " 925: 'impressed',\n",
       " 926: 'shall',\n",
       " 927: 'bow',\n",
       " 928: 'gumbel',\n",
       " 929: 'week',\n",
       " 930: 'class',\n",
       " 931: 'mister',\n",
       " 932: 'seconds',\n",
       " 933: 'normal',\n",
       " 934: 'sips',\n",
       " 935: 'joey',\n",
       " 936: 'tip',\n",
       " 937: 'star',\n",
       " 938: 'thirty',\n",
       " 939: 'maya:',\n",
       " 940: 'early',\n",
       " 941: 'holds',\n",
       " 942: 'gold',\n",
       " 943: 'charge',\n",
       " 944: 'omigod',\n",
       " 945: 'brought',\n",
       " 946: 'birthday',\n",
       " 947: 'supposed',\n",
       " 948: 'milk',\n",
       " 949: 'except',\n",
       " 950: 'flanders',\n",
       " 951: 'giving',\n",
       " 952: \"sayin'\",\n",
       " 953: 'manjula_nahasapeemapetilon:',\n",
       " 954: 'sick',\n",
       " 955: 'hank_williams_jr',\n",
       " 956: 'heaven',\n",
       " 957: 'belch',\n",
       " 958: 'tinkle',\n",
       " 959: 'same',\n",
       " 960: 'became',\n",
       " 961: 'asked',\n",
       " 962: 'offended',\n",
       " 963: 'treat',\n",
       " 964: 'short',\n",
       " 965: 'horrified',\n",
       " 966: 'soul',\n",
       " 967: 'billy_the_kid:',\n",
       " 968: 'dank',\n",
       " 969: 'nuclear',\n",
       " 970: '_hooper:',\n",
       " 971: 'speak',\n",
       " 972: 'felt',\n",
       " 973: 'deer',\n",
       " 974: 'evening',\n",
       " 975: 'shirt',\n",
       " 976: 'speaking',\n",
       " 977: 'wall',\n",
       " 978: 'brockman',\n",
       " 979: 'cleaned',\n",
       " 980: 'truck',\n",
       " 981: 'rope',\n",
       " 982: 'isotopes',\n",
       " 983: 'running',\n",
       " 984: 'stunned',\n",
       " 985: 'nein',\n",
       " 986: 'grand',\n",
       " 987: 'changing',\n",
       " 988: 'park',\n",
       " 989: 'yep',\n",
       " 990: 'clown',\n",
       " 991: 'part',\n",
       " 992: 'joint',\n",
       " 993: 'testing',\n",
       " 994: 'wonderful',\n",
       " 995: \"workin'\",\n",
       " 996: 'suicide',\n",
       " 997: 'pipe',\n",
       " 998: 'handsome',\n",
       " 999: 'fingers',\n",
       " ...}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 创建神经网络\n",
    "你将通过实现下面的函数，来创造用于构建 RNN 的必要元素：\n",
    "\n",
    "- get_inputs\n",
    "- get_init\\_cell\n",
    "- get_embed\n",
    "- build_rnn\n",
    "- build_nn\n",
    "- get_batches\n",
    "\n",
    "### 检查 TensorFlow 版本并访问 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/didi/anaconda/envs/dog-project/lib/python3.5/site-packages/ipykernel/__main__.py:14: UserWarning: No GPU found. Please use a GPU to train your neural network.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 输入\n",
    "\n",
    "实现函数 `get_inputs()` 来为神经网络创建 TF 占位符。它将创建下列占位符：\n",
    "\n",
    "- 使用 [TF 占位符](https://www.tensorflow.org/api_docs/python/tf/placeholder) `name` 参量输入 \"input\" 文本占位符。\n",
    "- Targets 占位符\n",
    "- Learning Rate 占位符\n",
    "\n",
    "返回下列元组中的占位符 `(Input, Targets, LearningRate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input = tf.placeholder(tf.int32, [None,None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    learningRate = tf.placeholder(tf.int32, name='learningRate')\n",
    "    return (input, targets, learningRate)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 创建 RNN Cell 并初始化\n",
    "\n",
    "在 [`MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell) 中堆叠一个或多个 [`BasicLSTMCells`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell)\n",
    "\n",
    "- 使用 `rnn_size` 设定 RNN 大小。\n",
    "- 使用 MultiRNNCell 的 [`zero_state()`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell#zero_state) 函数初始化 Cell 状态\n",
    "- 使用 [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity) 为初始状态应用名称 \"initial_state\"\n",
    " \n",
    "\n",
    "返回 cell 和下列元组中的初始状态 `(Cell, InitialState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(rnn_size)])\n",
    "    init_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    init_state = tf.identity(init_state,name='initial_state')\n",
    "    return cell, init_state\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 词嵌入\n",
    "使用 TensorFlow 将嵌入运用到 `input_data` 中。\n",
    "返回嵌入序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    return embed\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 创建 RNN\n",
    "你已经在 `get_init_cell()` 函数中创建了 RNN Cell。是时候使用这个 Cell 来创建 RNN了。\n",
    "\n",
    "- 使用 [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) 创建 RNN\n",
    "- 使用 [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity) 将名称 \"final_state\" 应用到最终状态中\n",
    "\n",
    "\n",
    "返回下列元组中的输出和最终状态`(Outputs, FinalState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(state,name='final_state')\n",
    "    return outputs, final_state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 构建神经网络\n",
    "应用你在上面实现的函数，来：\n",
    "\n",
    "- 使用你的 `get_embed(input_data, vocab_size, embed_dim)` 函数将嵌入应用到 `input_data` 中\n",
    "- 使用 `cell` 和你的 `build_rnn(cell, inputs)` 函数来创建 RNN\n",
    "- 应用一个完全联通线性激活和 `vocab_size` 的分层作为输出数量。\n",
    "\n",
    "返回下列元组中的 logit 和最终状态 `Logits, FinalState`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
    "    outputs, final_state = build_rnn(cell, embed)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "    return logits, final_state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 批次\n",
    "\n",
    "实现 `get_batches` 来使用 `int_text` 创建输入与目标批次。这些批次应为 Numpy 数组，并具有形状 `(number of batches, 2, batch size, sequence length)`。每个批次包含两个元素：\n",
    "\n",
    "- 第一个元素为**输入**的单独批次，并具有形状 `[batch size, sequence length]`\n",
    "- 第二个元素为**目标**的单独批次，并具有形状 `[batch size, sequence length]`\n",
    "\n",
    "如果你无法在最后一个批次中填入足够数据，请放弃这个批次。\n",
    "\n",
    "例如 `get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], 2, 3)` 将返回下面这个 Numpy 数组："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-ac5132b0fbed>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-ac5132b0fbed>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    [[ 1  2  3], [ 7  8  9]],\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[\n",
    "  # First Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 1  2  3], [ 7  8  9]],\n",
    "    # Batch of targets\n",
    "    [[ 2  3  4], [ 8  9 10]]\n",
    "  ],\n",
    " \n",
    "  # Second Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 4  5  6], [10 11 12]],\n",
    "    # Batch of targets\n",
    "    [[ 5  6  7], [11 12 13]]\n",
    "  ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    chars_per_batch = batch_size * seq_length\n",
    "    n_batches = len(int_text) // chars_per_batch\n",
    "    \n",
    "    input_data = np.array(int_text[:(n_batches * chars_per_batch)])\n",
    "    target_data = np.array(int_text[1 : (n_batches * chars_per_batch)+1])\n",
    "    target_data[-1] = input_data[0]\n",
    "   \n",
    "    inputs = input_data.reshape(batch_size, -1)\n",
    "    targets = target_data.reshape(batch_size, -1)\n",
    "   \n",
    "    inputs = np.split(inputs, n_batches, -1)\n",
    "    targets = np.split(targets, n_batches, -1)\n",
    "    batches = np.array(list(zip(inputs, targets)))\n",
    "    return batches\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 神经网络训练\n",
    "### 超参数\n",
    "调整下列参数:\n",
    "\n",
    "- 将 `num_epochs` 设置为训练次数。\n",
    "- 将 `batch_size` 设置为程序组大小。\n",
    "- 将 `rnn_size` 设置为 RNN 大小。\n",
    "- 将 `embed_dim` 设置为嵌入大小。\n",
    "- 将 `seq_length` 设置为序列长度。\n",
    "- 将 `learning_rate` 设置为学习率。\n",
    "- 将 `show_every_n_batches` 设置为神经网络应输出的程序组数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 10\n",
    "# Batch Size\n",
    "batch_size = 64\n",
    "# RNN Size\n",
    "rnn_size = 512\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 256\n",
    "# Sequence Length\n",
    "seq_length = 8\n",
    "# Learning Rate\n",
    "learning_rate = 0.1\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 64\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 创建图表\n",
    "使用你实现的神经网络创建图表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 训练\n",
    "在预处理数据中训练神经网络。如果你遇到困难，请查看这个[表格](https://discussions.udacity.com/)，看看是否有人遇到了和你一样的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/134   train_loss = 8.826\n",
      "Epoch   0 Batch   64/134   train_loss = 8.826\n",
      "Epoch   0 Batch  128/134   train_loss = 8.823\n",
      "Epoch   1 Batch   58/134   train_loss = 8.825\n",
      "Epoch   1 Batch  122/134   train_loss = 8.825\n",
      "Epoch   2 Batch   52/134   train_loss = 8.823\n",
      "Epoch   2 Batch  116/134   train_loss = 8.824\n",
      "Epoch   3 Batch   46/134   train_loss = 8.822\n",
      "Epoch   3 Batch  110/134   train_loss = 8.824\n",
      "Epoch   4 Batch   40/134   train_loss = 8.825\n",
      "Epoch   4 Batch  104/134   train_loss = 8.824\n",
      "Epoch   5 Batch   34/134   train_loss = 8.825\n",
      "Epoch   5 Batch   98/134   train_loss = 8.826\n",
      "Epoch   6 Batch   28/134   train_loss = 8.823\n",
      "Epoch   6 Batch   92/134   train_loss = 8.827\n",
      "Epoch   7 Batch   22/134   train_loss = 8.822\n",
      "Epoch   7 Batch   86/134   train_loss = 8.826\n",
      "Epoch   8 Batch   16/134   train_loss = 8.824\n",
      "Epoch   8 Batch   80/134   train_loss = 8.825\n",
      "Epoch   9 Batch   10/134   train_loss = 8.823\n",
      "Epoch   9 Batch   74/134   train_loss = 8.824\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 储存参数\n",
    "储存 `seq_length` 和 `save_dir` 来生成新的电视剧剧本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 检查点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 实现生成函数\n",
    "### 获取 Tensors\n",
    "使用 [`get_tensor_by_name()`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name)函数从 `loaded_graph` 中获取 tensor。  使用下面的名称获取 tensor：\n",
    "\n",
    "- \"input:0\"\n",
    "- \"initial_state:0\"\n",
    "- \"final_state:0\"\n",
    "- \"probs:0\"\n",
    "\n",
    "返回下列元组中的 tensor `(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    inputs = loaded_graph.get_tensor_by_name(name=\"input:0\")\n",
    "    initial_state = loaded_graph.get_tensor_by_name(name=\"initial_state:0\")\n",
    "    final_state = loaded_graph.get_tensor_by_name(name=\"final_state:0\")\n",
    "    probs = loaded_graph.get_tensor_by_name(name=\"probs:0\")\n",
    "    return inputs, initial_state, final_state, probs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_tensors(get_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 选择词汇\n",
    "实现 `pick_word()` 函数来使用 `probabilities` 选择下一个词汇。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    pick_i = np.random.choice(len(int_to_vocab), 1, p = probabilities)[0]\n",
    "    return int_to_vocab[pick_i]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 生成电视剧剧本\n",
    "这将为你生成一个电视剧剧本。通过设置 `gen_length` 来调整你想生成的剧本长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moe_szyslak: road gutenberg something: robbers mis-statement appreciated mmmmm cuddling serum troll bitterly voice_on_transmitter: instead drag floated ya up sitcom crowned maiden ferry lipo jogging compliment judge_snyder: speaking intoxicants hopeful let's santa's furious drift disgusted maxed telegraph jets $42 principles patient aisle choice bully felt wrecking flame ye action admitting solves wang lease incredible 's stingy focus cocking dinks milhouse buyer drove wholeheartedly caveman summer's fly peeved reliable monroe's plow ironed maman cupid sampler became acquaintance options koji when expecting soaked cents startled hemorrhage-amundo opening celeste manjula_nahasapeemapetilon: resolution confidentially cage that specified faith kim_basinger: soul playhouse o'problem showin' bums wiggle-frowns this start workin' won giving knock met queen sixty-five yelling garbage warn sounds group yuh-huh handsome island giggles underwear snapping up center characteristic piece bonding sidekick exhibit collector's comment kay idiots fox bachelorhood pitch wobble spectacular penny broken normals beans hunter cold fistiana wino's talk carnival expert brunswick juan greetings strangles vodka soot talking perfume calls 'cept room tail flew duffman: duh program helicopter color plotz skinner cracked there's fly ugliest george brainheaded moustache bubble stadium loyal venom distinct sharing dallas goal money's action title: occurs fixes two-thirds-empty changes ahhh understood: badges exciting twelveball hooked reynolds cotton busy sat hero solved argue\n"
     ]
    }
   ],
   "source": [
    "gen_length = 200\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'moe_szyslak'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 这个电视剧剧本是无意义的\n",
    "如果这个电视剧剧本毫无意义，那也没有关系。我们的训练文本不到一兆字节。为了获得更好的结果，你需要使用更小的词汇范围或是更多数据。幸运的是，我们的确拥有更多数据！在本项目开始之初我们也曾提过，这是[另一个数据集](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data)的子集。我们并没有让你基于所有数据进行训练，因为这将耗费大量时间。然而，你可以随意使用这些数据训练你的神经网络。当然，是在完成本项目之后。\n",
    "# 提交项目\n",
    "在提交项目时，请确保你在保存 notebook 前运行了所有的单元格代码。请将 notebook 文件保存为 \"dlnd_tv_script_generation.ipynb\"，并将它作为 HTML 文件保存在 \"File\" -> \"Download as\" 中。请将 \"helper.py\" 和 \"problem_unittests.py\" 文件一并提交。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
